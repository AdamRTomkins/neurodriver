{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Neurokernel on Amazon Compute Cloud GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show step by step how to run Neurokernel on Amazon Elastic Compute Cloud (Amazon EC2) GPU instances. This notebook assumes that you have already setup an account on Amazon Web Services (AWS) and that you are able to start an Amazon EC2 Linux Instance. For detailed tutorial on starting an Linux Instance, see [Getting Started with Amazon EC2 Linux Instances](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html). AWS currently provide free tier for new users for one year on t2.micro instances. If you have not used AWS before, you can try starting an linux instance free of charge to familiarize yourself with the cloud computing service.\n",
    "\n",
    "**Important**: GPU instances on Amazon EC2 always incur charges. Please be sure that you understand pricing structure on Amazon EC2. Pricing information can be found [here](http://aws.amazon.com/ec2/pricing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Preloaded Neurokernel Amazon Machine Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made available a public Amazon Machine Image (AMI) in which all packages required by Neurokernel are preloaded. Currently, AMI with the latest CUDA 7.5 is available. We will update the AMI from time to time as newer version of packages are released. Please follow the steps below to launch an GPU instance using the AMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, search in the public images for Neurokernel. Click on the one with AMI ID: ami-8bdd97e1. Then click Launch button on the top.\n",
    "<img src='files/files/ec2-ami-1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted to Step 2 of instance setup. \n",
    "Here, choose either g2.2xlarge for an instance with 1 GPU or g2.8xlarge for one with 4 GPUs.\n",
    "<img src='files/files/ec2-launch-step2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3: leave the default setting for instance details, or customize it according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 4, add storage with at least 8 GiB size. If you wish to keep the root storage, uncheck \"Delete on Termination\" box. Add additional storage as you need.\n",
    "<img src='files/files/ec2-launch-step4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 5, you can leave it as is or create a new tag for you instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 6, select an existing security group. Then, review and launch the instance. You can log into your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are logged in, actviate the Neurokernel environment by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source activate NK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now start using Neurokernel in ~/neurokernel. You can try the intro example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/neurokernel/examples/intro/data\n",
    "python gen_generic_lpu.py -s 0 -l lpu_0 generic_lpu_0.gexf.gz generic_lpu_0_input.h5                \n",
    "python gen_generic_lpu.py -s 1 -l lpu_1 generic_lpu_1.gexf.gz generic_lpu_1_input.h5\n",
    "cd ../\n",
    "python intro_demo.py --gpu_dev 0 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neurokernel from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to install NVIDIA Driver, CUDA and Neurokernel from scratch, you can following the guide below. These are the steps that were used to create the AMI mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a GPU Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your EC2 dashboard, click on \"instance\" on the navigation bar on the left and then click on the \"Launch Instance\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 1, choose Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-d05e75b8.\n",
    "<img src='files/files/ec2-launch-step1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 2, choose either g2.2xlarge for an instance with 1 GPU or g2.8xlarge for one with 4 GPUs.\n",
    "<img src='files/files/ec2-launch-step2.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3, leave the default setting for instance details, or customize it according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 4, add storage with at least 8 GiB size. If you wish to keep the root storage, uncheck \"Delete on Termination\" box. Add additional storage as you need.\n",
    "<img src='files/files/ec2-launch-step4.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 5, you can leave it as is or create a new tag for you instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 6, select an existing security group. Then, review and launch the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NVIDIA Drivers and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ubuntu AMI does not come with GPU driver installed. After you have launched the GPU instance, the first thing to do is to install NVIDIA drivers. This requires a series of commands. The following commands install 340.46 driver and CUDA 7.0. In principle, latest NVIDIA driver and CUDA library can be installed.\n",
    "\n",
    "**Important Note**: At various point, you will be prompted to answer [Y/n] when executing some of the following commands. Please make sure that you have installed all packages by answering \"y\" and make sure that you DO NOT see \"Abort\" at the end of output of each command. The best way to avoid this is to execute the commands one line at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt-get update\n",
    "sudo apt-get upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, you will be prompted \"What would you like to do about menu.lst?\". Select: \"keep the local version currently installed\". Wait until the upgrade is finished and execute the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt-get install build-essential\n",
    "sudo apt-get install linux-image-extra-virtual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted again with \"What would you like to do about menu.lst?\". Select again: \"keep the local version currently installed\". Wait until the end of the installation and execute the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo -e \"blacklist nouveau\\nblacklist lbm-nouveau\\noptions nouveau modeset=0\\nalias nouveau off\\nalias lbm-nouveau off\" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf\n",
    "echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf\n",
    "sudo update-initramfs -u\n",
    "sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server will reboot. Log in after it is up again, execute the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt-get install linux-source\n",
    "sudo apt-get install linux-headers-`uname -r`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to install NVIDIA Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir packages\n",
    "cd packages\n",
    "wget http://us.download.nvidia.com/XFree86/Linux-x86_64/352.63/NVIDIA-Linux-x86_64-352.63.run\n",
    "chmod u+x NVIDIA-Linux-x86_64-352.63.run\n",
    "sudo ./NVIDIA-Linux-x86_64-352.63.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of GPU driver starts here. Accept the license agreement. \"OK\" throughout the installation process. When prompted \"Would you like to run the nvidia-xconfig utility to automatically update your X configuration file so that the NVIDIA X driver will be used when you restart X?\", choose \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we install CUDA by the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo modprobe nvidia\n",
    "wget http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run\n",
    "chmod u+x cuda_7.5.18_linux.run\n",
    "./cuda_7.5.18_linux.run -extract=`pwd`\n",
    "sudo ./cuda-linux64-rel-7.5.18-19867135.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the license agreement appears, press \"q\" so you don't have to scroll down. Type in \"accept\" to accept the license agreement. Press Enter to use the default path. Enter \"no\" when asked to add desktop menu shortcuts. Enter \"yes\" for creating a symbolic link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, update your path variable by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo -e \"export PATH=/usr/local/cuda-7.5/bin:\\$PATH\\nexport LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64:\\$LD_LIBRARY_PATH\" | tee -a ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Neurokernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to install Neurokernel on Ubuntu is to use conda. We first install miniconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh\n",
    "chmod u+x Miniconda-latest-Linux-x86_64.sh\n",
    "./Miniconda-latest-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When prompt, accept the license agreement and choose default path for installing miniconda. Answer \"yes\" when asked \"Do you wish the installer to prepend the Miniconda2 install location\n",
    "to PATH in your /home/ubuntu/.bashrc ?\". After installation is complete, edit ~/.condarc file by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo -e \"channels:\\n- https://conda.binstar.org/neurokernel/channel/ubuntu1404\\n- defaults\" > ~/.condarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then create a new conda environment containing the packages required by Neurokernel by the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo apt-get install libibverbs1 libnuma1 libpmi0 libslurm26 libtorque2 libhwloc-dev git\n",
    "source ~/.bashrc\n",
    "conda create -n NK neurokernel_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the Neurokernel repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd\n",
    "rm -rf packages\n",
    "git clone https://github.com/neurokernel/neurokernel.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source activate NK\n",
    "cd ~/neurokernel\n",
    "python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test by running intro example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/neurokernel/examples/intro/data\n",
    "python gen_generic_lpu.py -s 0 -l lpu_0 generic_lpu_0.gexf.gz generic_lpu_0_input.h5                \n",
    "python gen_generic_lpu.py -s 1 -l lpu_1 generic_lpu_1.gexf.gz generic_lpu_1_input.h5\n",
    "cd ../\n",
    "python intro_demo.py --gpu_dev 0 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
